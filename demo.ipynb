{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import trange\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from core.config import default_config\n",
    "from core.extract_video_rgb import extract_video_rgb\n",
    "from core.image_segmentor import load_sam_auto_gen, segment_with_sam\n",
    "from core.mask_handler import MaskHandler\n",
    "from core.propagator import load_sam2_video_predictor_and_initial_state\n",
    "from core.task_handler import TaskHandler\n",
    "from core.utils import ID2RGBConverter, flatten_mask, viz_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: extract rgb into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/scratch/quanta/Experiments/Thesis/exps/prob_video_4_open_laptop/viz/rgb.mp4\"\n",
    "work_dir = Path(\n",
    "    \"/scratch/quanta/Experiments/Thesis/exps/sam2_full_video_track_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_video_rgb(\n",
    "    video_path=video_path,\n",
    "    save_workspace_dir=work_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Key frame image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = load_sam_auto_gen(\n",
    "    ckpt_pth=\"/scratch/quanta/Models/SAM/sam_vit_h_4b8939.pth\",\n",
    "    points_per_side=default_config['sam1_points_per_side'],\n",
    "    pred_iou_thresh=default_config['sam1_pred_iou_thrshold'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_with_sam(\n",
    "    rgb_dir=str(work_dir / \"data/rgb\"),\n",
    "    save_dir=str(work_dir / \"data/sam_1_seg\"),\n",
    "    sam_auto_gen=sam,\n",
    "    min_size=default_config['min_size'],\n",
    "    step=default_config['step'],\n",
    "    max_masks_per_frame=default_config['max_masks_per_frame'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sam\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use SAM2 to propagate and associate all masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_pth = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "ckpt_pth = \"/scratch/quanta/Models/SAM2/sam2.1_hiera_large.pt\"\n",
    "device = \"cuda:0\"\n",
    "video_predictor, inference_state = load_sam2_video_predictor_and_initial_state(\n",
    "    ckpt_pth=ckpt_pth,\n",
    "    rgb_jpg_dir=str(work_dir / \"data/rgb\"),\n",
    "    model_config=config_pth,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_handler = TaskHandler(\n",
    "    queue_dir=str(work_dir / \"data/sam_2_queue\"),\n",
    "    sam_mask_dir=str(work_dir / \"data/sam_1_seg\"),\n",
    "    save_mask_dir=str(work_dir / \"data/sam_2_track\"),\n",
    "    video_predictor=video_predictor,\n",
    "    inference_state=inference_state,\n",
    "    step=default_config['step'],\n",
    "    disappear_thresh=default_config['disappear_threshold'],\n",
    "    iou_thresh=default_config['iou_threshold'],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_handler.submit_initial_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_return = True\n",
    "while task_return is True:\n",
    "    torch.cuda.empty_cache()\n",
    "    task_return = task_handler.run_one_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load id_map and union find\n",
    "id_map = {}\n",
    "\n",
    "import json\n",
    "with open(str(work_dir / \"data/sam_2_queue/id_map.json\")) as f:\n",
    "    tmp_map = json.load(f)\n",
    "\n",
    "for i in tmp_map.keys():\n",
    "    j = i\n",
    "    while tmp_map[str(j)] != j:\n",
    "        j = tmp_map[str(j)]\n",
    "\n",
    "    id_map[int(i)] = j\n",
    "\n",
    "with open(str(work_dir / \"data/sam_2_queue/united_id_map.json\"), \"w\") as f:\n",
    "    json.dump(id_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = ID2RGBConverter()\n",
    "mask_handler = MaskHandler(str(work_dir / \"data/sam_2_track\"))\n",
    "\n",
    "viz_save_pth = work_dir / \"temp/viz_sam2_association\"\n",
    "viz_save_pth.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(task_handler.num_frames):\n",
    "    masks_data = mask_handler.load_masks(i)\n",
    "    obj_ids = [id_map[item['original_obj_id']] for item in masks_data]\n",
    "    viz_img = viz_mask(\n",
    "        flattened_mask=flatten_mask(\n",
    "            mask=masks_data,\n",
    "            object_id_list=obj_ids,\n",
    "        ),\n",
    "        converter=converter,\n",
    "    )\n",
    "    Image.fromarray(viz_img).save(str(viz_save_pth / \"{:06d}.png\".format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "\n",
    "(ffmpeg.input(\n",
    "    str(viz_save_pth / \"*.png\"),\n",
    "    pattern_type=\"glob\",\n",
    "    framerate=30,\n",
    ").output(\n",
    "    str(work_dir / \"viz/sam2_assotiation.mp4\")\n",
    ").run())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
