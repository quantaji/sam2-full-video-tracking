{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import trange\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from core.config import default_config\n",
    "from core.extract_video_rgb import extract_video_rgb\n",
    "from core.image_segmentor import load_sam_auto_gen, segment_with_sam\n",
    "from core.mask_handler import MaskHandler\n",
    "from core.propagator import load_sam2_video_predictor_and_initial_state\n",
    "from core.task_handler import TaskHandler\n",
    "from core.utils import ID2RGBConverter, flatten_mask, viz_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: extract rgb into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/scratch/quanta/Experiments/Thesis/exps/prob_video_4_open_laptop/viz/rgb.mp4\"\n",
    "work_dir = Path(\n",
    "    \"/scratch/quanta/Experiments/Thesis/exps/sam2_full_video_track_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 708/708 [00:08<00:00, 85.07it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_video_rgb(\n",
    "    video_path=video_path,\n",
    "    save_workspace_dir=work_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Key frame image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = load_sam_auto_gen(\n",
    "    ckpt_pth=\"/scratch/quanta/Models/SAM/sam_vit_h_4b8939.pth\",\n",
    "    points_per_side=default_config['sam1_points_per_side'],\n",
    "    pred_iou_thresh=default_config['sam1_pred_iou_thrshold'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [20:37<00:00,  8.71s/it]\n"
     ]
    }
   ],
   "source": [
    "segment_with_sam(\n",
    "    rgb_dir=str(work_dir / \"data/rgb\"),\n",
    "    save_dir=str(work_dir / \"data/sam_1_seg\"),\n",
    "    sam_auto_gen=sam,\n",
    "    min_size=default_config['min_size'],\n",
    "    step=default_config['step'],\n",
    "    max_masks_per_frame=default_config['max_masks_per_frame'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sam\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use SAM2 to propagate and associate all masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG):   1%|          | 5/708 [00:00<00:17, 39.78it/s]"
     ]
    }
   ],
   "source": [
    "config_pth = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "ckpt_pth = \"/scratch/quanta/Models/SAM2/sam2.1_hiera_large.pt\"\n",
    "device = \"cuda:0\"\n",
    "video_predictor, inference_state = load_sam2_video_predictor_and_initial_state(\n",
    "    ckpt_pth=ckpt_pth,\n",
    "    rgb_jpg_dir=str(work_dir / \"data/rgb\"),\n",
    "    model_config=config_pth,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_handler = TaskHandler(\n",
    "    queue_dir=str(work_dir / \"data/sam_2_queue\"),\n",
    "    sam_mask_dir=str(work_dir / \"data/sam_1_seg\"),\n",
    "    save_mask_dir=str(work_dir / \"data/sam_2_track\"),\n",
    "    video_predictor=video_predictor,\n",
    "    inference_state=inference_state,\n",
    "    step=default_config['step'],\n",
    "    disappear_thresh=default_config['disappear_threshold'],\n",
    "    iou_thresh=default_config['iou_threshold'],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142it [00:33,  4.21it/s]\n",
      "142it [00:17,  8.02it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 708/708 [00:24<00:00, 29.13it/s]\n"
     ]
    }
   ],
   "source": [
    "task_handler.submit_initial_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_return = True\n",
    "while task_return is True:\n",
    "    torch.cuda.empty_cache()\n",
    "    task_return = task_handler.run_one_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load id_map and union find\n",
    "id_map = {}\n",
    "\n",
    "import json\n",
    "with open(str(work_dir / \"data/sam_2_queue/id_map.json\")) as f:\n",
    "    tmp_map = json.load(f)\n",
    "\n",
    "for i in tmp_map.keys():\n",
    "    j = i\n",
    "    while tmp_map[str(j)] != j:\n",
    "        j = tmp_map[str(j)]\n",
    "\n",
    "    id_map[int(i)] = j\n",
    "\n",
    "with open(str(work_dir / \"data/sam_2_queue/united_id_map.json\"), \"w\") as f:\n",
    "    json.dump(id_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = ID2RGBConverter()\n",
    "mask_handler = MaskHandler(str(work_dir / \"data/sam_2_track\"))\n",
    "\n",
    "viz_save_pth = work_dir / \"temp/viz_sam2_association\"\n",
    "viz_save_pth.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 708/708 [15:49<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in trange(task_handler.num_frames):\n",
    "    masks_data = mask_handler.load_masks(i)\n",
    "    obj_ids = [id_map[item['original_obj_id']] for item in masks_data]\n",
    "    viz_img = viz_mask(\n",
    "        flattened_mask=flatten_mask(\n",
    "            mask=masks_data,\n",
    "            object_id_list=obj_ids,\n",
    "        ),\n",
    "        converter=converter,\n",
    "    )\n",
    "    Image.fromarray(viz_img).save(str(viz_save_pth / \"{:06d}.png\".format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with gcc 13.3.0 (conda-forge gcc 13.3.0-1)\n",
      "  configuration: --prefix=/home/quanta/.conda/envs/sam2 --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1727723455708/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1727723455708/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1727723455708/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1727723455708/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --enable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libopenh264 --enable-libdav1d --disable-gnutls --enable-libmp3lame --enable-libvpx --enable-libass --enable-pthreads --enable-vaapi --enable-libopenvino --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1727723455708/_build_env/bin/pkg-config\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "Input #0, image2, from '/scratch/quanta/Experiments/Thesis/exps/sam2_full_video_track_example/temp/viz_sam2_association/*.png':\n",
      "  Duration: 00:00:23.60, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/unknown/unknown), 1860x1860, 30 fps, 30 tbr, 30 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x56095c83c540] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x56095c83c540] profile High 4:4:4 Predictive, level 5.0, 4:4:4, 8-bit\n",
      "[libx264 @ 0x56095c83c540] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=48 lookahead_threads=8 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/scratch/quanta/Experiments/Thesis/exps/sam2_full_video_track_example/viz/sam2_assotiation.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf61.7.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 1860x1860, q=2-31, 30 fps, 15360 tbn\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.19.100 libx264\n",
      "      Side data:\n",
      "        cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/mp4 @ 0x56095c840880] video:29673KiB audio:0KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.032135%\n",
      "frame=  708 fps=160 q=-1.0 Lsize=   29682KiB time=00:00:23.53 bitrate=10332.4kbits/s speed=5.33x    \n",
      "[libx264 @ 0x56095c83c540] frame I:4     Avg QP:20.73  size:120500\n",
      "[libx264 @ 0x56095c83c540] frame P:179   Avg QP:25.27  size: 61619\n",
      "[libx264 @ 0x56095c83c540] frame B:525   Avg QP:32.53  size: 35947\n",
      "[libx264 @ 0x56095c83c540] consecutive B-frames:  1.1%  0.0%  0.0% 98.9%\n",
      "[libx264 @ 0x56095c83c540] mb I  I16..4: 26.5% 52.6% 20.9%\n",
      "[libx264 @ 0x56095c83c540] mb P  I16..4:  3.6%  3.7%  3.3%  P16..4:  7.4%  8.6%  5.0%  0.0%  0.0%    skip:68.3%\n",
      "[libx264 @ 0x56095c83c540] mb B  I16..4:  0.6%  0.8%  0.7%  B16..8: 15.3%  8.6%  2.7%  direct: 1.6%  skip:69.8%  L0:50.1% L1:45.7% BI: 4.2%\n",
      "[libx264 @ 0x56095c83c540] 8x8 transform intra:37.6% inter:8.5%\n",
      "[libx264 @ 0x56095c83c540] coded y,u,v intra: 15.8% 13.6% 13.2% inter: 3.6% 3.1% 2.6%\n",
      "[libx264 @ 0x56095c83c540] i16 v,h,dc,p: 53% 42%  4%  0%\n",
      "[libx264 @ 0x56095c83c540] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 22%  8% 69%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0x56095c83c540] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 22% 28% 33%  2%  2%  3%  4%  3%  3%\n",
      "[libx264 @ 0x56095c83c540] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x56095c83c540] ref P L0: 48.2%  5.5% 26.5% 19.7%\n",
      "[libx264 @ 0x56095c83c540] ref B L0: 74.2% 19.8%  6.0%\n",
      "[libx264 @ 0x56095c83c540] ref B L1: 88.1% 11.9%\n",
      "[libx264 @ 0x56095c83c540] kb/s:10299.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "(work_dir / \"viz\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "(ffmpeg.input(\n",
    "    str(viz_save_pth / \"*.png\"),\n",
    "    pattern_type=\"glob\",\n",
    "    framerate=30,\n",
    ").output(\n",
    "    str(work_dir / \"viz/sam2_assotiation.mp4\")\n",
    ").run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
